# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_data.load.ipynb (unless otherwise specified).

__all__ = ['Bucket', 'create_bucket', 'bucketify', 'detect_batch_to_samples', 'DetectDataLoader', 'old_do_call']

# Cell
import fastai2.imports
import fastcore.imports

# Cell
# def is_iter(o):
#     "Test whether `o` can be used in a `for` loop"
#     res = True
#     try: iter(o)
#     except TypeError: res = False
#     #Rank 0 tensors in PyTorch are not really iterable
#     return res and getattr(o,'ndim',1)
# fastai2.imports.is_iter = is_iter
# fastcore.imports.is_iter = is_iter

# Cell
from fastai2.vision.all import *

# Cell
class Bucket:
    def __init__(self, items): self.items = items
    def __repr__(self): return f'<{self.__class__.__name__}: {self.items.__repr__()}>'
#     def __getitem__(self, i): return L.__getitem__(self, i)
    def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else Bucket(self._get(idx))
    def _get(self, i):
        if is_indexer(i) or isinstance(i,slice): return getattr(self.items,'iloc',self.items)[i]
        i = mask2idxs(i)
        return (self.items.iloc[list(i)] if hasattr(self.items,'iloc')
                else self.items.__array__()[(i,)] if hasattr(self.items,'__array__')
                else [self.items[i_] for i_ in i])
#     def __iter__(self): raise TypeError(f"'{self.__class__.__name__}' object is not iterable")
    def __eq__(self, other): return self.items == other
    def __len__(self): return len(self.items)
    def tolist(self): return list(self.items)
    def map(self, f): return type(self)(type(self.items)(f(o) for o in self.items))
    def to_device(self, device): return type(self)(to_device(self.items))
    @property
    def shape(self): return (len(self.items),) # Needed for find_bs

# Cell
def create_bucket(items):
#     name = items[0].__class__.__name__ + 'Bucket'
#     return type(name, (Bucket,), {})
    return Bucket

# Cell
class _Buckets(dict):
    def __getitem__(self, k):
        try: return super().__getitem__(type(k[0]))(k)
        except KeyError:
            v = self[type(k[0])] = create_bucket(k)
            return v(k)

# Cell
_buckets = _Buckets()

# Cell
def bucketify(items): return _buckets[items]

# Cell
def _bucket_collate(t): return Tuple(bucketify(o) for o in zip(*t))
def _bucket_convert(t): raise NotImplementedError

# Cell
def detect_batch_to_samples(b, max_n=10):
    zipped = []
    for i in range(min(len(b[0]), max_n)):
        zipped.append(Tuple([o[i] for o in b]))
    return zipped
#     return L(b).zip()[:max_n]

# Cell
class DetectDataLoader(TfmdDL):
    def create_batch(self, b): return (_bucket_collate,_bucket_convert)[self.prebatched](b)

    def _decode_batch(self, b, max_n=9, full=True):
        f = self.after_item.decode
        f = compose(f, partial(getattr(self.dataset,'decode',noop), full = full))
        return L(detect_batch_to_samples(b, max_n=max_n)).map(f)

#     def _one_pass(self):
#         res = super()._one_pass()
#         self._types = {Tuple: [tuple, tuple]} # HACK
#         return res

    def show_batch(self, b=None, max_n=9, ctxs=None, show=True, unique=False, **kwargs):
        if unique:
            old_get_idxs = self.get_idxs
            self.get_idxs = lambda: Inf.zeros
        if b is None: b = self.one_batch()
        if not show: return self._pre_show_batch(b, max_n=max_n)
        show = show_batch[type(b[0][0]), type(b[1][0])]
        pb = self._pre_show_batch(b, max_n=max_n)
        show(*pb, ctxs=ctxs, max_n=max_n, **kwargs)
        if unique: self.get_idxs = old_get_idxs

# Cell
old_do_call = Transform._do_call
def _do_call(self, f, x, **kwargs):
    if isinstance(x, Bucket):
        _f = lambda o: retain_type(f(o, **kwargs), o, f.returns_none(o))
        return x if f is None else x.map(_f)
    return old_do_call(self, f, x, **kwargs)
Transform._do_call = _do_call